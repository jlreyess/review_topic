{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "### Los siguientes procesos, tokenizan, crean bigrams, trigrams y lemmatizan el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jlreyes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./data/interim/clean_data.pkl\", \"rb\") as input_file:\n",
    "    data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is one of the best hotels Ive ever stayed at with incredible staff '\n",
      " 'catering to your every need. I felt like family whilst I was there! I stayed '\n",
      " 'for 3 weeks during the pandemic and was so impressed with all the measures '\n",
      " 'the hotel have put in place to keep everyone safe and well. They give you a '\n",
      " 'box of masks, gloves and a bottle of hand sanitiser upon arrival, all the '\n",
      " 'staff wear masks and gloves and the rooms and everything else are cleaned to '\n",
      " 'the highest standard. Plus the prices theyve been offering during this time '\n",
      " 'are incredible value for money! Definitely would recommend and hope to be '\n",
      " 'back once travel reopens. Thank you so much for looking after me so well '\n",
      " 'until I could return home!']\n"
     ]
    }
   ],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'one', 'of', 'the', 'best', 'hotels', 'ive', 'ever', 'stayed', 'at', 'with', 'incredible', 'staff', 'catering', 'to', 'your', 'every', 'need', 'felt', 'like', 'family', 'whilst', 'was', 'there', 'stayed', 'for', 'weeks', 'during', 'the', 'pandemic', 'and', 'was', 'so', 'impressed', 'with', 'all', 'the', 'measures', 'the', 'hotel', 'have', 'put', 'in', 'place', 'to', 'keep', 'everyone', 'safe', 'and', 'well', 'they', 'give', 'you', 'box', 'of', 'masks', 'gloves', 'and', 'bottle', 'of', 'hand', 'sanitiser', 'upon', 'arrival', 'all', 'the', 'staff', 'wear', 'masks', 'and', 'gloves', 'and', 'the', 'rooms', 'and', 'everything', 'else', 'are', 'cleaned', 'to', 'the', 'highest', 'standard', 'plus', 'the', 'prices', 'theyve', 'been', 'offering', 'during', 'this', 'time', 'are', 'incredible', 'value', 'for', 'money', 'definitely', 'would', 'recommend', 'and', 'hope', 'to', 'be', 'back', 'once', 'travel', 'reopens', 'thank', 'you', 'so', 'much', 'for', 'looking', 'after', 'me', 'so', 'well', 'until', 'could', 'return', 'home'], ['everything', 'about', 'this', 'hotel', 'was', 'awesome', 'the', 'staff', 'made', 'me', 'feel', 'welcomed', 'from', 'the', 'minute', 'arrived', 'until', 'the', 'day', 'departed', 'the', 'view', 'from', 'the', 'dining', 'area', 'with', 'the', 'mountains', 'in', 'the', 'background', 'was', 'spectacular', 'the', 'grounds', 'were', 'manicured', 'to', 'perfection', 'with', 'flowering', 'plants', 'in', 'every', 'direction', 'in', 'the', 'center', 'of', 'the', 'pool', 'courtyard', 'area', 'was', 'the', 'largest', 'bougainvillea', 'have', 'ever', 'seen', 'it', 'was', 'blue', 'and', 'in', 'full', 'bloom', 'estimate', 'it', 'was', 'feet', 'plus', 'in', 'height', 'fresh', 'flower', 'arrangements', 'in', 'the', 'reception', 'and', 'dining', 'area', 'the', 'room', 'was', 'well', 'appointed', 'clean', 'and', 'comfortable', 'travel', 'extensively', 'for', 'business', 'and', 'pleasure', 'this', 'in', 'one', 'of', 'the', 'best', 'hotels', 'have', 'ever', 'stayed', 'at', 'truly', 'an', 'amazing', 'experience', 'the', 'city', 'of', 'antigua', 'is', 'beautiful', 'with', 'grand', 'plaza', 'preserved', 'buildings', 'and', 'churches'], ['our', 'tour', 'group', 'stayed', 'here', 'for', 'two', 'nights', 'the', 'view', 'in', 'the', 'picture', 'is', 'exactly', 'like', 'the', 'one', 'took', 'with', 'my', 'own', 'camera', 'our', 'room', 'was', 'spacious', 'had', 'fireplace', 'water', 'provided', 'in', 'their', 'jug', 'container', 'it', 'was', 'very', 'clean', 'staff', 'was', 'very', 'friendly', 'and', 'hospitable']]\n"
     ]
    }
   ],
   "source": [
    "print(data_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Construimos modelos de bigrams y trigrams\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    # trigrams se forma construyendo bigrams en los datos + bigrams 1\n",
    "    # Aplicamos el conjunto de bigrams/trigrams a nuestros documentos\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(4, 1), (8, 1), (10, 1), (13, 1), (19, 1), (36, 1), (39, 1), (41, 1), (44, 1), (49, 1), (51, 1), (52, 1), (53, 3), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1)], [(4, 1), (36, 1), (39, 1), (41, 1), (91, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1)], [(10, 1), (19, 1), (21, 1), (39, 1), (88, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1)], [(0, 1), (4, 1), (5, 1), (36, 1), (44, 1), (49, 2), (81, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos stopwords\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Formamos bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Inicializamos el modelo 'en_core_web_lg' con las componentes de POS únicamente\n",
    "\n",
    "\n",
    "# Lematizamos preservando únicamente noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Creamos diccionario\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[1:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./data/interim/tokenize_corpus.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(corpus, output_file)\n",
    "with open(r\"./data/interim/tokenize_id2word.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(id2word, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
